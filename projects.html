<!DOCTYPE html>
<html>

<head>
<title>Tyler Hughes - Projects</title>
<style>html{overflow-y:scroll}body{background-color:#d3d3d3;font-family:Verdana,Geneva,sans-serif;font-weight:400;color:#000}h1,h2,h3,h4,h5,h6{}p{font-size:11pt;line-height:1.5em}#container{width:1000px;margin:0 auto 0 auto;background-color:#fff}#header{padding:13px 20px 10px 20px}#header .name{display:inline-table}#header .title{display:inline-table}#header h1{font-family:Arial-Black,Gadget,sans-serif;font-size:30pt;font-weight:bold;margin:5px 0 5px 0}#header p{font-family:Arial-Black,Gadget,sans-serif;font-size:16pt;margin:5px 0 5px 0}#header a{text-decoration:none;color:#000}#header a:hover{color:#000}div#page_body{width:1000px}div#index_page_contents,div#index_picture_frame,div#left_menu,div#page_contents,div#contact_page_contents,div#contact_picture_frame{padding:10px 10px 10px 10px;min-height:100px;float:left}div#page_contents{width:920px;padding:10px 40px 10px 40px;background-color:#fff}div#index_page_contents{width:550px;padding:10px 10px 10px 40px;background-color:#fff}div#index_picture_frame{width:380px;background-color:#fff}div#contact_page_contents{width:450px;padding:10px 10px 10px 40px;background-color:#fff}div#contact_picture_frame{width:480px;background-color:#fff}div#top_menu{background-color:#333;width:1000px;display:inline-table;padding:0 0 0 0}div#top_menu ul{list-style-type:none;padding:0 0 0 0;margin:0 0 0 0}div#top_menu li{text-align:center;padding:7px 0 7px 0;width:20%;display:inline;float:left}div#top_menu li:hover{background-color:#999}div#top_menu li.active{background-color:#999}div#top_menu a{background-color:#0f0;text-decoration:none;color:#fff}div#footer{font-size:15pt;clear:both;padding:10px 10px 10px 10px;background-color:#eee}div#footer .copyright{font-size:9pt;text-align:right;margin:0 0 0 0}img.noborder{border:0;margin:0;padding:0}a.icon{text-decoration:none}a{text-decoration:none}p.tab{margin-left:30px}p.tabtab{margin-left:60px}div#index_page_contents li{font-size:11pt;line-height:1.5em}</style>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
</head>

<body>
  <div id="container">

    <div id="header">
      <span class="name">
	<h1><a href="index.html">Tyler William Hughes</a></h1>
	<p class="title"> PhD Student, Stanford University</p>
      </span>
    </div>

    <div id="top_menu">
      <ul>
	<a href="index.html"><li>Home</li></a>      	
	<a href="research.html"><li>Research</li></a>
    <a href="courses.html"><li>Courses</li></a>    
	<a href="projects.html"><li class="active">Projects</li></a>
    <a href="other.html"><li>Other</li></a>    
      </ul>
    </div>
    <div id="page_body">
      <div id="page_contents">
	<h2>These are a mix of class projects and personal projects outside of my scientific research. </h2>
    
    <h4>Accelerating Symbolic Regression with Deep Learning:</h4>
    <img src="images/symbolic_regression_flow.png" alt="Tree Branch" style="width:604px;height:154px;display: block; margin:0 auto;"/>
    <p>
The goal of symbolic regression is to generate a function that describes a given set of data-points.  This function can, generally, include undetermined constants that may be fit later.  Typical approaches to symbolic regression involve global optimization techniques based on genetic algorithms [1,2,3], that search a subset of the entire space of possible equations to find the best fit.  This approach is cumbersome and does not utilize any of the features or structure inherent in the original data.  
    </p>
    <p>
        In this work, we approach symbolic as a modified machine translation problem, where our goal is to translate the original dataset into an equation using a mixture of machine learning techniques.  We first train a neural network to extract features from the dataset by performing a fit and stripping out the learned parameters from the network.  We then train a recurrent neural network model (using a tree of LSTMs) to decode this feature vector into an equation tree.  Further processing of this equation can be performed to fit constants and evaluate accuracy vs. simplicity.
    </p>
        <img src="images/LSTM_tree.png" alt="Tree Branch" style="width:404px;height:304px;display: block; margin:0 auto;"/>
    <p>
        We achieved modest results in predicting equations, although there is much room for experimentation and improvement.  This work presents a fresh approach to the problem of symbolic regression and may allow for potential increases in predictive power, reliability, and speed compared to previous solutions.
    </p>

    <p>
This was the final project for CS221 (AI)
    </p>

      <a href="https://web.stanford.edu/class/cs221/restricted/reports/twhughes/final.pdf">Paper</a>  <a href="https://github.com/twhughes/Symbolic-Regression">   Code</a>

    </p>
    <ul>
        <li>[1] Josh Bongard and Hod Lipson. Automated reverse engineering of nonlinear dynamical systems. Proceedings
of the National Academy of Sciences, 104(24):9943–9948, 2007.</li>
        <li>[2] John Duffy and Jim Engle-Warnick. Using symbolic regression to infer strategies from experimental data.
In Evolutionary computation in Economics and Finance, pages 61–82. Springer, 2002.</li>
        <li>[3] Wouter Minnebo, Sean Stijven, and Katya Vladislavleva. Empowering knowledge computing with variable
selection, 2011.</li>    
    </ul>


<h4>Pedestrian Trajectory Tracking:</h4>
    <p>
We trained a number of sequential models to predict the trajectory of pedestrians as they move through a scene and interact with one another. Our model could be used to predict movements of crowds of people and vehicles given an overhead image of a scene. This may have potential applications in helping to make public spaces less susceptible to crowding or accidents, improving control of autonomous vehicles, and video surveillance.  We are leveraging a part of the Stanford Drone dataset, which contains a large number of overhead images of crowded spaces on Stanford campus:(http://cvgl.stanford.edu/projects/uav_data/). 
    </p>
    <p>
        We trained an LSTM model to predict the positions and velocities of each pedestrian at a later time in the scene.  This model was fed the previous positions and velocities of the pedestrian, along with 
        <ul>
            <li> a visual map of where other pedestrians were in the scene
            <li> a visual map of where the pedestrian is located in the scene
            <li> an array representing the segmented image (grass, sidewalk, road, etc.)
        </ul>
These arrays were fed through a pre-trained CNN before inputting into the LSTM.
    </p>
        <img src="images/trajectory_tracking.png" alt="Tree Branch" style="width:404px;height:304px;display: block; margin:0 auto"/>
    <p>
        Our model showed slight performance enhancement over a linear trajectory model.  We ran into issues because the complexity of our model was so high that we could not train on a reasonable amount of time.
    </p>
    <p>
This was the final project for CS230 (Deep Learning)
    </p>
     <a href="https://github.com/twhughes/Trajectory-Prediction">Code & Notebook</a>
    </p>

<h4>Reinforcement Learning for Optimizing Optical Structures</h4>
    <p>
Our project focused on training a reinforcement learning algorithm to optimize optical structures.  We wish to input a desired device performance  metric and return a procedure for constructing this device.  In our case, we took a 1D stack of dielectric (glass) slabs as our structure and tried to reproduce a desired reflection spectrum.  Our optimization procedure was modeled as a Markov Decision Process (MDP) where we tried to learn the transition probabilities and rewards associated with each device state and optimization step.  
    </p>
    <img src="images/reinforcement_learning.png" alt="Tree Branch" style="width:404px;height:204px; display: block; margin:0 auto"/>
    <p>
        Our algorithm was able to perform well for few layers (< 3) but was not able to learn for many layered stacks, limiting its usefulness.  We believe this is because we were not able to effectively explore the exponentially large exploration space.
    </p>
    <p>
This was the final project for CS229 (Machine Learning)
    </p>
     <a href="https://pdfs.semanticscholar.org/c056/82e5aa5a20c6ecb463119bd73a55ccfc8197.pdf">Paper</a>
    </p>

<h4>Numerical E&M Packages</h4>
    <p>
Out of several numerical techniques for solving electromagnetic problems, the finite-difference time-domain (FDTD) and finite-difference frequency-domain (FDFD) methods are among the most widely used.   FDTD is used to compute the time-dependence of the electromagnetic fields in a system, such as under illumination by an optical pulse.  FDFD is used to examine the steady state behavior of an optical system (at a single frequency).  FDTD requires performing updates on a matrix based on the time-dependent Maxwell’s equations, whereas FDFD requires solving a large, sparse, linear system.
    </p>
<p>
    I have written solvers for these methods in a variety of languages, including:
</p>
    <ul>
        <li> <a href="https://github.com/twhughes/Faraday-FDFD">Julia</a>
        <li> <a href="https://github.com/twhughes/FDTD_JS">Javascript</a>
        <li> <a href="https://github.com/twhughes/Finite-Difference-Frequency-Domain">Python</a>
        <li> <a href="https://github.com/twhughes/DLA-Structure-Optimization">MATLAB</a>
    </ul>

    <div id="footer">
      <p class="copyright">
	&#169 Tyler Hughes, 2017
      </p>
    </div>
  </div>
</body>
</html>